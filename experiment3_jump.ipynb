{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY303hveKMyF"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vsPlfkl6MaYZ",
        "outputId": "4cd033d2-fad6-4449-f824-5d028abe4713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aT_lrr3QKMyH",
        "outputId": "d3a5b44a-dd3f-4608-8d47-fd06f8fda7ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from drive.MyDrive.ATNLP.crispy_fortnight_exp3.transformer import Transformer\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from drive.MyDrive.ATNLP.crispy_fortnight_exp3.accuracy import sequence_level_accuracy, token_lvl_accuracy\n",
        "\n",
        "# %%\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on {device}\")\n",
        "\n",
        "# Experiment 2 & 3 Hyperparameters\n",
        "EMB_DIM = 128\n",
        "N_LAYERS = 2\n",
        "N_HEADS = 8\n",
        "FORWARD_DIM = 256\n",
        "DROPOUT = 0.15\n",
        "LEARNING_RATE = 2e-4\n",
        "GRAD_CLIP = 1\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "# Optimizer: AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytiv2bdFKMyL"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VaGE7wFwKMyM"
      },
      "outputs": [],
      "source": [
        "class TasksData(Dataset):\n",
        "    def __init__(self, data_dir, file, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.file = file\n",
        "        text_file = os.path.join(data_dir, file)\n",
        "\n",
        "        data_dict = {\"src\": [], \"tgt\": []}\n",
        "\n",
        "        with open(text_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                src = line.split('OUT:')[0]\n",
        "                src = src.split('IN:')[1].strip()\n",
        "                tgt = line.split('OUT:')[1].strip()\n",
        "\n",
        "                data_dict['src'].append(src)\n",
        "                data_dict['tgt'].append(tgt)\n",
        "\n",
        "        self.data = pd.DataFrame(data_dict)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.data['src'].iloc[idx] + ' <EOS>'\n",
        "        tgt = '<SOS> ' + self.data['tgt'].iloc[idx] + ' <EOS>'\n",
        "        return src, tgt\n",
        "\n",
        "def create_vocab(dataset):\n",
        "    vocab = set()\n",
        "\n",
        "    for sample in dataset:\n",
        "        vocab.update(sample.split())\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqUaPduQKMyM"
      },
      "source": [
        "# Training, saving, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yXLEm6ikKMyN",
        "outputId": "28945392-ccb8-4590-e1d5-7ed251d1217b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [1/10], Loss: 0.4671: 100%|██████████| 917/917 [00:18<00:00, 48.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [1/10], Loss: 0.7237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [2/10], Loss: 0.4454: 100%|██████████| 917/917 [00:18<00:00, 49.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [2/10], Loss: 0.4230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [3/10], Loss: 0.2546: 100%|██████████| 917/917 [00:18<00:00, 49.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [3/10], Loss: 0.3309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [4/10], Loss: 0.1396: 100%|██████████| 917/917 [00:18<00:00, 50.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [4/10], Loss: 0.2103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [5/10], Loss: 0.1241: 100%|██████████| 917/917 [00:18<00:00, 49.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [5/10], Loss: 0.1363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [6/10], Loss: 0.1796: 100%|██████████| 917/917 [00:18<00:00, 50.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [6/10], Loss: 0.1014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [7/10], Loss: 0.0572: 100%|██████████| 917/917 [00:18<00:00, 49.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [7/10], Loss: 0.0775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [8/10], Loss: 0.0639: 100%|██████████| 917/917 [00:18<00:00, 50.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [8/10], Loss: 0.0627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [9/10], Loss: 0.0294: 100%|██████████| 917/917 [00:18<00:00, 49.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [9/10], Loss: 0.0494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Epoch [10/10], Loss: 0.0294: 100%|██████████| 917/917 [00:18<00:00, 49.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp 1, Epoch [10/10], Loss: 0.0417\n"
          ]
        }
      ],
      "source": [
        "# composed_commands = [0] # [, 1, 2, 4, 8, 16, 32]\n",
        "composed_commands = 1\n",
        "rep_number = 2\n",
        "\n",
        "exp_losses = []\n",
        "exp_accuracies = []\n",
        "exp_times = []\n",
        "token_acc_results = []\n",
        "seq_scc_results = []\n",
        "num_epochs = 10\n",
        "\n",
        "print(f\"EXPERIMENT {composed_commands}\")\n",
        "train_data = TasksData(data_dir='/content/drive/MyDrive/ATNLP/data/Experiment-3/data/1_composed_command/train', file=f'tasks_train_addprim_complex_jump_num{composed_commands}_rep{rep_number}.txt')\n",
        "test_data = TasksData(data_dir='/content/drive/MyDrive/ATNLP/data/Experiment-3/data/1_composed_command/test', file=f'tasks_test_addprim_complex_jump_num{composed_commands}_rep{rep_number}.txt')\n",
        "\n",
        "# creating source and target vocab - and word2idx\n",
        "src_train_data = [src for src, tgt in train_data]\n",
        "vocab_train_src = create_vocab(src_train_data)\n",
        "tgt_train_data = [tgt for src, tgt in train_data]\n",
        "vocab_train_tgt = create_vocab(tgt_train_data)\n",
        "word2idx_src = {w: idx + 1 for (idx, w) in enumerate(vocab_train_src)}\n",
        "word2idx_src['<PAD>'] = 0\n",
        "word2idx_tgt= {w: idx + 1 for (idx, w) in enumerate(vocab_train_tgt)}\n",
        "word2idx_tgt['<PAD>'] = 0\n",
        "\n",
        "# custom collate function\n",
        "def custom_collate_fn(batch):\n",
        "    padded_src = pad_sequence([torch.tensor([word2idx_src[w] for w in src.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
        "    padded_tgt = pad_sequence([torch.tensor([word2idx_tgt[w] for w in tgt.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
        "    return padded_src, padded_tgt\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(word2idx_src),\n",
        "    tgt_vocab_size=len(word2idx_tgt),\n",
        "    src_pad_idx=word2idx_src['<PAD>'],\n",
        "    tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
        "    emb_dim=EMB_DIM,\n",
        "    num_layers=N_LAYERS,\n",
        "    num_heads=N_HEADS,\n",
        "    forward_dim=FORWARD_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=MAX_LEN,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# define the optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_tgt['<PAD>'])\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, (src, tgt) in (pbar := tqdm(enumerate(train_loader), total=len(train_loader))):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # inference\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # some sexy reshaping\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # loss calculation + backward pass\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        pbar.set_description(f'Composed {composed_commands} commands, Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
        "\n",
        "        # optimizer step and loss accumulation\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # after one epoch\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    losses.append(avg_epoch_loss)\n",
        "    print(f'Composed {composed_commands} commands, Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "# save the model\n",
        "checkpoint_path = f\"exp3_jump_num{composed_commands}_epoch{num_epochs}_v2.pth\"\n",
        "torch.save(\n",
        "    {'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'losses': losses\n",
        "}, checkpoint_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_tmp = []\n",
        "seq_tmp = []\n",
        "MAX_STEP = len(test_loader) # // 5\n",
        "\n",
        "for step, (src_batch, tgt_batch) in (pbar := tqdm(enumerate(test_loader), total=len(test_loader))):\n",
        "    if step > MAX_STEP:\n",
        "        break\n",
        "    for src, tgt in zip(src_batch, tgt_batch):\n",
        "        src = src.unsqueeze(0).to(device)\n",
        "        true_tgt = tgt.unsqueeze(0).to(device)\n",
        "        tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
        "\n",
        "        iterations = MAX_LEN - 1\n",
        "        pred_sequence = [tgt.item()]\n",
        "\n",
        "        for i in range(iterations):\n",
        "            with torch.no_grad():\n",
        "                output = model.forward(src, tgt)\n",
        "                predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
        "                next_token = predictions.argmax(-1).item()\n",
        "\n",
        "                pred_sequence.append(next_token)\n",
        "                tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
        "\n",
        "                # stop if end of sequence\n",
        "                if next_token == word2idx_tgt['<EOS>']:\n",
        "                    break\n",
        "\n",
        "        token_acc = token_lvl_accuracy(word2idx_tgt, true_tgt, tgt)\n",
        "        seq_acc = sequence_level_accuracy(true_tgt, tgt, word2idx_tgt)\n",
        "        token_tmp.append(token_acc)\n",
        "        seq_tmp.append(seq_acc)\n",
        "        # print(f'ground t: {true_tgt}')\n",
        "        # print(f'predicted: {tgt}')\n",
        "        # print(m)\n",
        "        # print()\n",
        "    # l += 1\n",
        "\n",
        "    pbar.set_description(f'Composed {composed_commands} commands{\"\\n\"}Token Acc: {sum(token_tmp)/len(token_tmp):.4f}, Seq Acc: {sum(seq_tmp)/len(seq_tmp):.4f}')\n",
        "print(\"\")\n",
        "print(sum(token_tmp)/len(token_tmp))\n",
        "print(sum(seq_tmp)/len(seq_tmp))\n",
        "\n",
        "token_acc_results.append(token_tmp)\n",
        "seq_scc_results.append(seq_tmp)"
      ],
      "metadata": {
        "id": "O-apl8aYQGq0",
        "outputId": "2dce42f8-b659-460d-a51b-3bd633684e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exp 1, Token Acc: 0.6203, Seq Acc: 0.0297: 100%|██████████| 482/482 [09:47<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0.6202677628171834\n",
            "0.02972096041531473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}