{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer import Transformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('mps')\n",
    "\n",
    "# EXPERIMENT 2:\n",
    "EMB_DIM = 128\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 8\n",
    "FORWARD_DIM = 256\n",
    "DROPOUT = 0.15\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_CLIP = 1\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"On {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0: DataLoader and Preprocessing\n",
    "class TasksData(Dataset):\n",
    "    def __init__(self, data_dir, file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.file = file\n",
    "        text_file = os.path.join(data_dir, file)\n",
    "\n",
    "        data_dict = {\"src\": [], \"tgt\": []}\n",
    "\n",
    "        with open(text_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                src = line.split('OUT:')[0]\n",
    "                src = src.split('IN:')[1].strip()\n",
    "                tgt = line.split('OUT:')[1].strip()\n",
    "\n",
    "                data_dict['src'].append(src)\n",
    "                data_dict['tgt'].append(tgt)\n",
    "\n",
    "        self.data = pd.DataFrame(data_dict)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data['src'].iloc[idx] + ' <EOS>'\n",
    "        tgt = '<SOS> ' + self.data['tgt'].iloc[idx] + ' <EOS>'\n",
    "        return src, tgt\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    vocab = set()\n",
    "\n",
    "    for sample in dataset:\n",
    "        vocab.update(sample.split())\n",
    "    return vocab\n",
    "\n",
    "# %%\n",
    "# creating datasets\n",
    "train_data = TasksData(data_dir='data/experiment2', file='tasks_train_length.txt')\n",
    "test_data = TasksData(data_dir='data/experiment2', file='tasks_test_length.txt')\n",
    "\n",
    "#creating source and target vocab\n",
    "src_train_data = [src for src, tgt in train_data]\n",
    "vocab_train_src = create_vocab(src_train_data)\n",
    "\n",
    "tgt_train_data = [tgt for src, tgt in train_data]\n",
    "vocab_train_tgt = create_vocab(tgt_train_data)\n",
    "\n",
    "# we need to do word2idx to map the words to indexes. Bc the input for nn.Embedding has to be numbers\n",
    "# since nn.Embdding has different weights in input andoutput embedding the same index will not be encoded to the same vector\n",
    "word2idx_src = {w: idx + 1 for (idx, w) in enumerate(vocab_train_src)}\n",
    "word2idx_src['<PAD>'] = 0\n",
    "\n",
    "word2idx_tgt= {w: idx + 1 for (idx, w) in enumerate(vocab_train_tgt)}\n",
    "word2idx_tgt['<PAD>'] = 0\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    #input: batch of sentences\n",
    "    # tokenize, word2idx, pad\n",
    "    padded_src = pad_sequence([torch.tensor([word2idx_src[w] for w in src.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "    padded_tgt = pad_sequence([torch.tensor([word2idx_tgt[w] for w in tgt.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    return padded_src, padded_tgt\n",
    "\n",
    "# %%\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(word2idx_src),\n",
    "    tgt_vocab_size=len(word2idx_tgt),\n",
    "    src_pad_idx=word2idx_src['<PAD>'],\n",
    "    tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
    "    emb_dim=EMB_DIM,\n",
    "    num_layers=N_LAYERS,\n",
    "    num_heads=N_HEADS,\n",
    "    forward_dim=FORWARD_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lvl_accuracy(gt, pred):\n",
    "    \"\"\"\n",
    "    gt = ground truth sequence\n",
    "    pred = predicted sequence\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    \n",
    "    # get start and end\n",
    "    eos_idx = word2idx_tgt['<EOS>']\n",
    "    sos_idx = word2idx_tgt['<SOS>']\n",
    "    # print(eos_idx)\n",
    "    # print(sos_idx)\n",
    "    pred = pred[-1]\n",
    "\n",
    "\n",
    "    gt = gt[-1]\n",
    "\n",
    "    # index of <SOS> and <EOS> tokens of the predicted sequence\n",
    "    pred_start = 0\n",
    "    pred_end = len(pred) if (eos_idx not in pred) else (pred == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # index of <SOS> and <EOS> tokens of the ground truth sequence\n",
    "    gt_start = (gt == sos_idx).nonzero(as_tuple=True)[0].item()\n",
    "    gt_end = (gt == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # slicing\n",
    "    gt = gt[gt_start+1 : gt_end]\n",
    "    pred = pred[pred_start+1 : pred_end]\n",
    "\n",
    "    longer = gt if len(gt) > len(pred) else pred\n",
    "    shorter = pred if len(gt) > len(pred) else gt\n",
    "\n",
    "    longest_len = len(longer)\n",
    "\n",
    "    shorter = torch.nn.functional.pad(shorter, (0, longest_len - len(shorter)), \"constant\", 0)\n",
    "\n",
    "    correct = sum(longer == shorter)\n",
    "    # print(longer)\n",
    "    # print(shorter)\n",
    "    # print(correct)\n",
    "    return int(correct) / len(shorter) # same length as longer\n",
    "\n",
    "\n",
    "def sequence_level_accuracy(gt, pred):\n",
    "\n",
    "    # get start and end\n",
    "    eos_idx = word2idx_tgt['<EOS>']\n",
    "    sos_idx = word2idx_tgt['<SOS>']\n",
    "    # print(eos_idx)\n",
    "    # print(sos_idx)\n",
    "    pred = pred[-1]\n",
    "    gt = gt[-1]\n",
    "\n",
    "    # index of <SOS> and <EOS> tokens of the predicted sequence\n",
    "    pred_start = 0\n",
    "    pred_end = len(pred) if (eos_idx not in pred) else (pred == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # index of <SOS> and <EOS> tokens of the ground truth sequence\n",
    "    gt_start = (gt == sos_idx).nonzero(as_tuple=True)[0].item()\n",
    "    gt_end = (gt == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # slicing\n",
    "    gt = gt[gt_start+1 : gt_end]\n",
    "    pred = pred[pred_start+1 : pred_end]\n",
    "\n",
    "    if len(gt) != len(pred):\n",
    "        return 0\n",
    "\n",
    "    if sum(gt == pred) == len(gt):\n",
    "        return 1\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1062 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarl/UniStuff/ATNLP/crispy-fortnight/transformer.py:187: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /croot/pytorch_1681837265408/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  K_transposed = K.T.permute(3, 1, 0, 2)\n",
      "Epoch [1/1], Loss: 0.5721: 100%|██████████| 1062/1062 [02:56<00:00,  6.01it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_tgt['<PAD>'])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "accuraacy = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, (src, tgt) in (pbar := tqdm(enumerate(train_loader), total=len(train_loader))):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output = model(src, tgt)\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "checkpoint_path = f\"transformer_exp1_15.pth\"\n",
    "torch.save(\n",
    "    {'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7e6e303fbe50>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the word-to-index mapping\n",
    "idx2word_src = {idx: w for w, idx in word2idx_src.items()}\n",
    "idx2word_tgt = {idx: w for w, idx in word2idx_tgt.items()}\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    return ' '.join(idx2word[idx] for idx in indices if idx in idx2word and idx != word2idx_src['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_tgt['<SOS>'])\n",
    "print(word2idx_tgt['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLANATION: I had to make length functions thta return the length of the sequence without the special tokens to get the keys for the dictionaries\n",
    "# One of them uses the word2idx_src and the other uses the word2idx_tgt to remove the special tokens and return the length\n",
    "\n",
    "def length_tgt(seq):\n",
    "    pad_token = word2idx_tgt['<PAD>']\n",
    "    sos_token = word2idx_tgt['<SOS>']\n",
    "    eos_token = word2idx_tgt['<EOS>']\n",
    "    \n",
    "    # seq is a tensor of numbers clean it form all pad, sos, or eos tokens\n",
    "    seq = seq[seq != pad_token]\n",
    "    seq = seq[seq != sos_token]\n",
    "    seq = seq[seq != eos_token]\n",
    "    \n",
    "    return len(seq)\n",
    "\n",
    "\n",
    "def length_src(seq):\n",
    "    pad_token = word2idx_src['<PAD>']\n",
    "    eos_token = word2idx_src['<EOS>']\n",
    "    \n",
    "    # seq is a tensor of numbers clean it form all pad, sos, or eos tokens\n",
    "    seq = seq[seq != pad_token]\n",
    "    seq = seq[seq != eos_token]\n",
    "    \n",
    "    return len(seq)\n",
    "\n",
    "\n",
    "# decoded_sentences = [idx2word_src[w] for w in [ 6,  5, 11, 14,  8,  0,  0,  0,  0,  0]]\n",
    "# print(decoded_sentences)\n",
    "\n",
    "# print(length_src(torch.tensor([ 6,  5, 11, 14,  8,  0,  0,  0,  0,  0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLANATION: This is the same as when you have last seen it I only changed the calculation of the lengths of the sequences to use as keys\n",
    "#checkpoint_path = \"transformer_exp2_test.pth\"\n",
    "ckp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(ckp['model_state_dict'])\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "src, tgt = next(iter(test_loader))\n",
    "# print(src.shape, tgt.shape)\n",
    "\n",
    "avg_token = []\n",
    "avg_seq = []\n",
    "gt_token_avgs = {}\n",
    "command_token_avgs = {}\n",
    "\n",
    "# avg_token = []\n",
    "# avg_seq = []\n",
    "\n",
    "n_batches = len(test_loader)\n",
    "\n",
    "for src_batch, tgt_batch in test_loader:\n",
    "    \n",
    "    for src, tgt in zip(src_batch, tgt_batch):\n",
    "        len_gt = length_tgt(tgt)\n",
    "        len_command = length_src(src)\n",
    "        if len_command == 3 or len_command == 5:\n",
    "            print(src)\n",
    "\n",
    "        src = src.unsqueeze(0).to(device)\n",
    "        true_tgt = tgt.unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "        \n",
    "        iterations = MAX_LEN - 1\n",
    "        pred_sequence = [tgt.item()]\n",
    "        # print(tgt.shape)\n",
    "        # print(src.shape, true_tgt.shape)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            with torch.no_grad():\n",
    "                output = model.forward(src, tgt)\n",
    "                predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "                next_token = predictions.argmax(-1).item()\n",
    "\n",
    "                pred_sequence.append(next_token)\n",
    "                tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Stop if end of sequence\n",
    "                if next_token == word2idx_tgt['<EOS>']:\n",
    "                    break                \n",
    "\n",
    "        tok_lvl_acc = token_lvl_accuracy(true_tgt, tgt)\n",
    "        seq_lvl_acc = sequence_level_accuracy(true_tgt, tgt)\n",
    "        avg_token.append(tok_lvl_acc)\n",
    "        avg_seq.append(seq_lvl_acc)\n",
    "\n",
    "        if len_gt in gt_token_avgs:\n",
    "            gt_token_avgs[len_gt].append(tok_lvl_acc)\n",
    "        else:\n",
    "            gt_token_avgs[len_gt] =[tok_lvl_acc]\n",
    "            \n",
    "        if len_command in command_token_avgs:\n",
    "            command_token_avgs[len_command].append(tok_lvl_acc)\n",
    "        else:\n",
    "            command_token_avgs[len_command] =[tok_lvl_acc]    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([8, 9])\n",
      "dict_keys([24])\n",
      "{8: 0.16777481049134588, 9: 0.16866141732283466}\n",
      "{24: 0.16794104927225006}\n",
      "Token level accuracy: 0.16794104927225006\n",
      "Sequence level accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#EXPLANATION: This prints out all relevant data we need for the results and plots \n",
    "# (also prints out the keys for the dictionaries not necessary but to sanity check)\n",
    "print(command_token_avgs.keys())\n",
    "print(gt_token_avgs.keys())    \n",
    "\n",
    "command_tokenavgs_avg = {k: sum(v) / len(v) for k, v in command_token_avgs.items()}\n",
    "gt_tokenavgs_avg = {k: sum(v) / len(v) for k, v in gt_token_avgs.items()}\n",
    "\n",
    "#TODO FOR JAN: Paste the printouts below to the first cell below Plotting without oracle\n",
    "print(f\"command_tokenavgs_avg = {command_tokenavgs_avg}\")\n",
    "print(f\"gt_tokenavgs_avg = {gt_tokenavgs_avg}\")\n",
    "\n",
    "print(f\"tkn_lvl_acc = {sum(avg_token) / len(avg_token)}\")\n",
    "print(f\"seq_lvl acc = {sum(avg_seq) / len(avg_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLANATION: here we added the oracle to the code. If the model puts out an <EOS> token before the ground truth sequence ends\n",
    "# it will choose the second most probable token instead of the <EOS> token.\n",
    "# This is done by checking if the predicted sequence is shorter than the ground truth sequence.\n",
    "# Further, we need to collect more data this time. We additonal collect the sequence level accuracies for the ground truth and the command sequences. \n",
    "\n",
    "#checkpoint_path = \"transformer_exp2_test.pth\"\n",
    "ckp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(ckp['model_state_dict'])\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "src, tgt = next(iter(test_loader))\n",
    "# print(src.shape, tgt.shape)\n",
    "\n",
    "avg_token = []\n",
    "avg_seq = []\n",
    "gt_token_avgs = {}\n",
    "command_token_avgs = {}\n",
    "gt_seq_avgs = {}\n",
    "command_seq_avgs = {}\n",
    "\n",
    "n_batches = len(test_loader)\n",
    "# l = 0\n",
    "for src_batch, tgt_batch in test_loader:\n",
    "    for src, tgt in zip(src_batch, tgt_batch):\n",
    "        len_gt = length_tgt(tgt)\n",
    "        len_command = length_src(src)\n",
    "\n",
    "        src = src.unsqueeze(0).to(device)\n",
    "        true_tgt = tgt.unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "        \n",
    "        iterations = MAX_LEN - 1\n",
    "        pred_sequence = [tgt.item()]\n",
    "        # print(tgt.shape)\n",
    "        # print(src.shape, true_tgt.shape)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            with torch.no_grad():\n",
    "                output = model.forward(src, tgt)\n",
    "                predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "                next_token = predictions.argmax(-1).item()\n",
    "                \n",
    "                #EXPLANATION: Oracle implementation                \n",
    "                if next_token == word2idx_tgt['<EOS>'] and len(pred_sequence) < len_gt + 1:\n",
    "                    next_token = predictions.argsort(-1, descending=True).squeeze()[1].item()\n",
    "                    \n",
    "                pred_sequence.append(next_token)\n",
    "                tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
    "            \n",
    "                \n",
    "                # Stop if end of sequence\n",
    "                if next_token == word2idx_tgt['<EOS>']:\n",
    "                    break                \n",
    "\n",
    "        tok_lvl_acc = token_lvl_accuracy(true_tgt, tgt)\n",
    "        seq_lvl_acc = sequence_level_accuracy(true_tgt, tgt)\n",
    "        avg_token.append(tok_lvl_acc)\n",
    "        avg_seq.append(seq_lvl_acc)\n",
    "        \n",
    "        if len_gt in gt_token_avgs:\n",
    "            gt_token_avgs[len_gt].append(tok_lvl_acc)\n",
    "        else:\n",
    "            gt_token_avgs[len_gt] =[tok_lvl_acc]\n",
    "            \n",
    "        if len_command in command_token_avgs:\n",
    "            command_token_avgs[len_command].append(tok_lvl_acc)\n",
    "        else:\n",
    "            command_token_avgs[len_command] =[tok_lvl_acc]\n",
    "        \n",
    "        if len_gt in gt_seq_avgs:\n",
    "            gt_seq_avgs[len_gt].append(seq_lvl_acc)\n",
    "        else:\n",
    "            gt_seq_avgs[len_gt] = [seq_lvl_acc]\n",
    "            \n",
    "        if len_command in command_seq_avgs:\n",
    "            command_seq_avgs[len_command].append(seq_lvl_acc)\n",
    "        else:\n",
    "            command_seq_avgs[len_command] =[seq_lvl_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'command_token_avgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcommand_token_avgs\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(gt_token_avgs\u001b[38;5;241m.\u001b[39mkeys())    \n\u001b[1;32m      4\u001b[0m command_tokenavgs_avg \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28msum\u001b[39m(v) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m command_token_avgs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'command_token_avgs' is not defined"
     ]
    }
   ],
   "source": [
    "#EXPLANATION: print out of data for the results and plots (+keys)\n",
    "print(command_token_avgs.keys())\n",
    "print(gt_token_avgs.keys())    \n",
    "\n",
    "command_tokenavgs_avg = {k: sum(v) / len(v) for k, v in command_token_avgs.items()}\n",
    "gt_tokenavgs_avg = {k: sum(v) / len(v) for k, v in gt_token_avgs.items()}\n",
    "\n",
    "command_seqavgs_avg = {k: sum(v) / len(v) for k, v in command_seq_avgs.items()}\n",
    "gt_seqavgs_avg = {k: sum(v) / len(v) for k, v in gt_seq_avgs.items()}\n",
    "\n",
    "#TODO FOR JAN: Paste the printouts below to the first cell below Plotting with oracle \n",
    "\n",
    "print(f\"command_tokenavgs_avg = {command_tokenavgs_avg}\")\n",
    "print(f\"gt_tokenavgs_avg = {gt_tokenavgs_avg}\")\n",
    "\n",
    "print(f\"command_seqavgs_avg = {command_seqavgs_avg}\")\n",
    "print(f\"gt_seqavgs_avg = {gt_seqavgs_avg}\")\n",
    "\n",
    "print(f\"tkn_lvl_acc = {sum(avg_token) / len(avg_token)}\")\n",
    "print(f\"seq_lvl_acc = {sum(avg_seq) / len(avg_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting with oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO FOR JAN: Paste them here\n",
    "\n",
    "# dict_keys([8, 9, 7, 3, 6, 5])\n",
    "# dict_keys([24, 25, 26, 27, 28, 30, 32, 33, 36, 40, 48])\n",
    "# command_tokenavgs_avg = {8: 0.6997872084670901, 9: 0.685847075549836, 7: 0.7673854647961234, 3: 0.8691666666666665, 6: 0.7689884382228773, 5: 0.6838500319723386}\n",
    "# gt_tokenavgs_avg = {24: 0.7738561478434809, 25: 0.6041005387027976, 26: 0.7540051630501513, 27: 0.7349516674485894, 28: 0.7823963286686172, 30: 0.7669131460182892, 32: 0.7587093871302877, 33: 0.7187184583499463, 36: 0.7723759731898171, 40: 0.7152218868137427, 48: 0.7524735392955254}\n",
    "# command_seqavgs_avg = {8: 0.11430921052631579, 9: 0.005208333333333333, 7: 0.22432432432432434, 3: 0.5, 6: 0.2591145833333333, 5: 0.25}\n",
    "# gt_seqavgs_avg = {24: 0.09226190476190477, 25: 0.078125, 26: 0.25390625, 27: 0.27901785714285715, 28: 0.27901785714285715, 30: 0.2690972222222222, 32: 0.14955357142857142, 33: 0.19140625, 36: 0.125, 40: 0.0546875, 48: 0.0}\n",
    "# token_level_acc =  0.7374888862176663\n",
    "# sequence_level_acc: 0.18852040816326532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token plots\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# # plot the average accuracy for each command length\n",
    "# axs[0].bar(list(command_tokenavgs_avg.keys()), list(command_tokenavgs_avg.values()))\n",
    "# axs[0].set_xlabel(\"Command Length (in words)\")\n",
    "# axs[0].set_ylabel(\"Accuracy on New Commands\")\n",
    "# axs[0].set_title(\"Token-Level Accuracy by Command Length\")\n",
    "# axs[0].set_xticks([4, 6, 7, 8, 9])\n",
    "\n",
    "# # plot the average accuracy for each ground-truth sequence length\n",
    "# axs[1].bar(list(gt_tokenavgs_avg.keys()), list(gt_tokenavgs_avg.values()))\n",
    "# axs[1].set_xlabel(\"Ground-Truth Action Sequence Length (in words)\")\n",
    "# axs[1].set_ylabel(\"Accuracy on New Commands (%)\")\n",
    "# axs[1].set_title(\"Token-Level Accuracy by Action Sequence Length\")\n",
    "# axs[1].set_xticks([25, 30, 35, 40, 45, 50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence plots\n",
    "\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# # plot the average accuracy for each command length\n",
    "# axs[0].bar(list(command_seqavgs_avg.keys()), list(command_seqavgs_avg.values()))\n",
    "# axs[0].set_xlabel(\"Command Length (in words)\")\n",
    "# axs[0].set_ylabel(\"Accuracy on New Commands\")\n",
    "# axs[0].set_title(\"Sequence-Level Accuracy by Command Length\")\n",
    "# axs[0].set_xticks([4, 6, 7, 8, 9])\n",
    "\n",
    "# # plot the average accuracy for each ground-truth sequence length\n",
    "# axs[1].bar(list(gt_seqavgs_avg.keys()), list(gt_seqavgs_avg.values()))\n",
    "# axs[1].set_xlabel(\"Ground-Truth Action Sequence Length (in words)\")\n",
    "# axs[1].set_ylabel(\"Accuracy on New Commands (%)\")\n",
    "# axs[1].set_title(\"Sequence-Level Accuracy by Action Sequence Length\")\n",
    "# axs[1].set_xticks([25, 30, 35, 40, 45, 50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting without oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO FOR JAN: Paste them here\n",
    "\n",
    "# dict_keys([8, 9, 7, 3, 6, 5])\n",
    "# dict_keys([24, 25, 26, 27, 28, 30, 32, 33, 36, 40, 48])\n",
    "\n",
    "# command_avgs_avg = {8: 0.6230199421957834, 9: 0.7345682457010582, 7: 0.44055249508374283, 3: 0.4270833333333333, 6: 0.4297355769230738, 5: 0.4008506944444439}\n",
    "# gt_avgs_avg = {24: 0.8767727229780791, 25: 0.618293899524219, 26: 0.43359374999999883, 27: 0.4742063492063489, 28: 0.4733122860531186, 30: 0.41446759259259464, 32: 0.5496651785714286, 33: 0.41477272727272746, 36: 0.4509548611111111, 40: 0.4425781249999994, 48: 0.27587890625}\n",
    "# token_level_acc = 0.5068159217168267\n",
    "# seq_level_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token plots\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# # plot the average accuracy for each command length\n",
    "# axs[0].bar(list(command_avgs_avg.keys()), list(command_avgs_avg.values()))\n",
    "# axs[0].set_xlabel(\"Command Length (in words)\")\n",
    "# axs[0].set_ylabel(\"Accuracy on New Commands\")\n",
    "# axs[0].set_title(\"Token-Level Accuracy by Command Length\")\n",
    "# axs[0].set_xticks([4, 6, 7, 8, 9])\n",
    "\n",
    "# # plot the average accuracy for each ground-truth sequence length\n",
    "# axs[1].bar(list(gt_avgs_avg.keys()), list(gt_avgs_avg.values()))\n",
    "# axs[1].set_xlabel(\"Ground-Truth Action Sequence Length (in words)\")\n",
    "# axs[1].set_ylabel(\"Accuracy on New Commands (%)\")\n",
    "# axs[1].set_title(\"Token-Level Accuracy by Action Sequence Length\")\n",
    "# axs[1].set_xticks([25, 30, 35, 40, 45, 50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
