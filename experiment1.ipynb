{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer import Transformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('mps')\n",
    "# EXPERIMENT 1:\n",
    "EMB_DIM = 128\n",
    "N_LAYERS = 1\n",
    "N_HEADS = 8\n",
    "FORWARD_DIM = 512\n",
    "DROPOUT = 0.05\n",
    "LEARNING_RATE = 7e-4\n",
    "BATCH_SIZE = 64\n",
    "GRAD_CLIP = 1\n",
    "MAX_LEN = 128 # ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"On {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0: DataLoader and Preprocessing\n",
    "class TasksData(Dataset):\n",
    "    def __init__(self, data_dir, file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.file = file\n",
    "        text_file = os.path.join(data_dir, file)\n",
    "\n",
    "        data_dict = {\"src\": [], \"tgt\": []}\n",
    "\n",
    "        with open(text_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                src = line.split('OUT:')[0]\n",
    "                src = src.split('IN:')[1].strip()\n",
    "                tgt = line.split('OUT:')[1].strip()\n",
    "\n",
    "                data_dict['src'].append(src)\n",
    "                data_dict['tgt'].append(tgt)\n",
    "\n",
    "        self.data = pd.DataFrame(data_dict)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data['src'].iloc[idx] + ' <EOS>'\n",
    "        tgt = '<SOS> ' + self.data['tgt'].iloc[idx] + ' <EOS>'\n",
    "        return src, tgt\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    vocab = set()\n",
    "\n",
    "    for sample in dataset:\n",
    "        vocab.update(sample.split())\n",
    "    return vocab\n",
    "\n",
    "# %%\n",
    "# creating datasets\n",
    "train_data = TasksData(data_dir='./data', file='tasks_train_simple.txt')\n",
    "test_data = TasksData(data_dir='./data', file='tasks_test_simple.txt')\n",
    "\n",
    "#creating source and target vocab\n",
    "src_train_data = [src for src, tgt in train_data]\n",
    "vocab_train_src = create_vocab(src_train_data)\n",
    "\n",
    "tgt_train_data = [tgt for src, tgt in train_data]\n",
    "vocab_train_tgt = create_vocab(tgt_train_data)\n",
    "\n",
    "# we need to do word2idx to map the words to indexes. Bc the input for nn.Embedding has to be numbers\n",
    "# since nn.Embdding has different weights in input andoutput embedding the same index will not be encoded to the same vector\n",
    "word2idx_src = {w: idx + 1 for (idx, w) in enumerate(vocab_train_src)}\n",
    "word2idx_src['<PAD>'] = 0\n",
    "\n",
    "word2idx_tgt= {w: idx + 1 for (idx, w) in enumerate(vocab_train_tgt)}\n",
    "word2idx_tgt['<PAD>'] = 0\n",
    "\n",
    "# We need Vocabulary size without padding\n",
    "# word2idx\n",
    "# padding\n",
    "#vocabulary and word2idx\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    #input: batch of sentences\n",
    "    # tokenize, word2idx, pad\n",
    "    padded_src = pad_sequence([torch.tensor([word2idx_src[w] for w in src.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "    padded_tgt = pad_sequence([torch.tensor([word2idx_tgt[w] for w in tgt.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    return padded_src, padded_tgt\n",
    "\n",
    "# %%\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(word2idx_src),\n",
    "    tgt_vocab_size=len(word2idx_tgt),\n",
    "    src_pad_idx=word2idx_src['<PAD>'],\n",
    "    tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
    "    emb_dim=EMB_DIM,\n",
    "    num_layers=N_LAYERS,\n",
    "    num_heads=N_HEADS,\n",
    "    forward_dim=FORWARD_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Level Acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lvl_accuracy(gt, pred):\n",
    "    correct = 0\n",
    "    i = 0\n",
    "\n",
    "    # get start and end\n",
    "    eos_idx = word2idx_tgt['<EOS>']\n",
    "    sos_idx = word2idx_tgt['<SOS>']\n",
    "    # print(eos_idx)\n",
    "    # print(sos_idx)\n",
    "    pred = pred[-1]\n",
    "    gt = gt[-1]\n",
    "\n",
    "    pred_start = (pred == sos_idx).nonzero(as_tuple=True)[0].item()\n",
    "    pred_end = (pred == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    gt_start = (gt == sos_idx).nonzero(as_tuple=True)[0].item()\n",
    "    gt_end = (gt == eos_idx).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # slicing\n",
    "    gt = gt[gt_start+1 : gt_end]\n",
    "    pred = pred[pred_start+1 : pred_end]\n",
    "\n",
    "    longer = gt if len(gt) > len(pred) else pred\n",
    "    shorter = pred if len(gt) > len(pred) else gt\n",
    "\n",
    "    longest_len = len(longer)\n",
    "\n",
    "    shorter = torch.nn.functional.pad(shorter, (0, longest_len - len(shorter)), \"constant\", 0)\n",
    "\n",
    "    correct = sum(longer == shorter)\n",
    "    # print(longer)\n",
    "    # print(shorter)\n",
    "    # print(correct)\n",
    "    return int(correct) / len(shorter) # same length as longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.6061: 100%|██████████| 262/262 [00:03<00:00, 68.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = Transformer(\n",
    "#     src_vocab_size=len(word2idx_src),\n",
    "#     tgt_vocab_size=len(word2idx_tgt),\n",
    "#     src_pad_idx=word2idx_src['<PAD>'],\n",
    "#     tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
    "#     emb_dim=EMB_DIM,\n",
    "#     num_layers=N_LAYERS,\n",
    "#     num_heads=N_HEADS,\n",
    "#     forward_dim=FORWARD_DIM,\n",
    "#     dropout=DROPOUT,\n",
    "#     max_len=MAX_LEN,\n",
    "# ).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_tgt['<PAD>'])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "losses = []\n",
    "accuraacy = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, (src, tgt) in (pbar := tqdm(enumerate(train_loader), total=len(train_loader))):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output = model(src, tgt)\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "checkpoint_path = f\"transformer_exp1_1.pth\"\n",
    "torch.save(\n",
    "    {'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the word-to-index mapping\n",
    "idx2word_src = {idx: w for w, idx in word2idx_src.items()}\n",
    "idx2word_tgt = {idx: w for w, idx in word2idx_tgt.items()}\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    return ' '.join(idx2word[idx] for idx in indices if idx in idx2word and idx != word2idx_src['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKEN LEVEL ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"transformer_exp1_100.pth\"\n",
    "ckp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(ckp['model_state_dict'])\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "def inference(input_seq: str):\n",
    "    model.eval()\n",
    "    pass\n",
    "\n",
    "model.eval()\n",
    "\n",
    "src, true_tgt = next(iter(test_loader))\n",
    "print(src.shape)\n",
    "src = src[0].unsqueeze(0).to(device)\n",
    "true_tgt = true_tgt[0].unsqueeze(0).to(device)\n",
    "# src, true_tgt = next(iter(test_loader))[0][0].unsqueeze(0).to(device)\n",
    "\n",
    "print(src)\n",
    "tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "# tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "\n",
    "# print(f'tgt {tgt.shape}')\n",
    "# print(f'src {src.shape}')\n",
    "iterations = 20\n",
    "pred_sequence = [tgt.item()]\n",
    "# print(pred_sequence)\n",
    "\n",
    "for i in range(iterations):\n",
    "    with torch.no_grad():\n",
    "        # print(tgt)\n",
    "        output = model.forward(src, tgt)\n",
    "        predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "        # print(\"argmax:\", predictions.argmax(-1).shape)\n",
    "        next_token = predictions.argmax(-1).item()\n",
    "\n",
    "\n",
    "        # next_token_tensor = torch.tensor([[next_token]]).to(device)\n",
    "        # tgt = torch.cat([tgt, next_token_tensor], dim=1)\n",
    "        \n",
    "        pred_sequence.append(next_token)\n",
    "        tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
    "\n",
    "        if next_token == word2idx_tgt['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        # print(tgt.shape)\n",
    "        # print(\"tgt\", tgt)\n",
    "\n",
    "print(f'ground t: {true_tgt}')\n",
    "print(f'predicted: {tgt}')\n",
    "print(token_lvl_accuracy(true_tgt, tgt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_tgt['<SOS>'])\n",
    "print(word2idx_tgt['<EOS>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magnu\\AppData\\Local\\Temp\\ipykernel_34076\\1514923549.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckp = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground t: tensor([[4, 3, 3, 3, 3, 3, 3, 6, 6, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "predicted: tensor([[4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8]], device='cuda:0')\n",
      "0.14285714285714285\n",
      "\n",
      "ground t: tensor([[4, 3, 2, 3, 2, 3, 7, 3, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "predicted: tensor([[4, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n",
      "         2, 3, 2, 3, 2, 3, 2, 3, 2, 8]], device='cuda:0')\n",
      "0.1875\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(src, tgt)\n\u001b[0;32m     31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m pred_sequence\u001b[38;5;241m.\u001b[39mappend(next_token)\n\u001b[0;32m     35\u001b[0m tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pred_sequence)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f\"transformer_exp1_1.pth\"\n",
    "ckp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(ckp['model_state_dict'])\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "src, tgt = next(iter(test_loader))\n",
    "# print(src.shape, tgt.shape)\n",
    "avg = []\n",
    "\n",
    "# l = 0\n",
    "\n",
    "for src_batch, tgt_batch in test_loader:\n",
    "    # if l > 20:\n",
    "    #     break\n",
    "    for src, tgt in zip(src_batch, tgt_batch):\n",
    "        # print(src)\n",
    "        # \n",
    "        src = src.unsqueeze(0).to(device)\n",
    "        true_tgt = tgt.unsqueeze(0).to(device)\n",
    "        tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "        \n",
    "        iterations = MAX_LEN\n",
    "        pred_sequence = [tgt.item()]\n",
    "        # print(tgt.shape)\n",
    "        # print(src.shape, true_tgt.shape)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            with torch.no_grad():\n",
    "                output = model.forward(src, tgt)\n",
    "                predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "                next_token = predictions.argmax(-1).item()\n",
    "\n",
    "                pred_sequence.append(next_token)\n",
    "                tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Stop if end of sequence\n",
    "                if next_token == word2idx_tgt['<EOS>']:\n",
    "                    break\n",
    "\n",
    "        m = token_lvl_accuracy(true_tgt, tgt)\n",
    "        avg.append(m)\n",
    "        print(f'ground t: {true_tgt}')\n",
    "        print(f'predicted: {tgt}')\n",
    "        print(m)\n",
    "        print()\n",
    "    # l += 1\n",
    "    \n",
    "print(sum(avg)/len(avg))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
