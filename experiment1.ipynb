{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer import Transformer\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('mps')\n",
    "# EXPERIMENT 1:\n",
    "EMB_DIM = 128\n",
    "N_LAYERS = 1\n",
    "N_HEADS = 8\n",
    "FORWARD_DIM = 512\n",
    "DROPOUT = 0.05\n",
    "LEARNING_RATE = 7e-4\n",
    "BATCH_SIZE = 64\n",
    "GRAD_CLIP = 1\n",
    "MAX_LEN = 128 # ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0: DataLoader and Preprocessing\n",
    "class TasksData(Dataset):\n",
    "    def __init__(self, data_dir, file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.file = file\n",
    "        text_file = os.path.join(data_dir, file)\n",
    "\n",
    "        data_dict = {\"src\": [], \"tgt\": []}\n",
    "\n",
    "        with open(text_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                src = line.split('OUT:')[0]\n",
    "                src = src.split('IN:')[1].strip()\n",
    "                tgt = line.split('OUT:')[1].strip()\n",
    "\n",
    "                data_dict['src'].append(src)\n",
    "                data_dict['tgt'].append(tgt)\n",
    "\n",
    "        self.data = pd.DataFrame(data_dict)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data['src'].iloc[idx] + ' <EOS>'\n",
    "        tgt = '<SOS> ' + self.data['tgt'].iloc[idx] + ' <EOS>'\n",
    "        return src, tgt\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    vocab = set()\n",
    "\n",
    "    for sample in dataset:\n",
    "        vocab.update(sample.split())\n",
    "    return vocab\n",
    "\n",
    "# %%\n",
    "# creating datasets\n",
    "train_data = TasksData(data_dir='./data', file='tasks_train_simple.txt')\n",
    "test_data = TasksData(data_dir='./data', file='tasks_test_simple.txt')\n",
    "\n",
    "#creating source and target vocab\n",
    "src_train_data = [src for src, tgt in train_data]\n",
    "vocab_train_src = create_vocab(src_train_data)\n",
    "\n",
    "tgt_train_data = [tgt for src, tgt in train_data]\n",
    "vocab_train_tgt = create_vocab(tgt_train_data)\n",
    "\n",
    "# we need to do word2idx to map the words to indexes. Bc the input for nn.Embedding has to be numbers\n",
    "# since nn.Embdding has different weights in input andoutput embedding the same index will not be encoded to the same vector\n",
    "word2idx_src = {w: idx + 1 for (idx, w) in enumerate(vocab_train_src)}\n",
    "word2idx_src['<PAD>'] = 0\n",
    "\n",
    "word2idx_tgt= {w: idx + 1 for (idx, w) in enumerate(vocab_train_tgt)}\n",
    "word2idx_tgt['<PAD>'] = 0\n",
    "\n",
    "# We need Vocabulary size without padding\n",
    "# word2idx\n",
    "# padding\n",
    "#vocabulary and word2idx\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    #input: batch of sentences\n",
    "    # tokenize, word2idx, pad\n",
    "    padded_src = pad_sequence([torch.tensor([word2idx_src[w] for w in src.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "    padded_tgt = pad_sequence([torch.tensor([word2idx_tgt[w] for w in tgt.split()]) for src, tgt in batch], batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    return padded_src, padded_tgt\n",
    "\n",
    "# %%\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(word2idx_src),\n",
    "    tgt_vocab_size=len(word2idx_tgt),\n",
    "    src_pad_idx=word2idx_src['<PAD>'],\n",
    "    tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
    "    emb_dim=EMB_DIM,\n",
    "    num_layers=N_LAYERS,\n",
    "    num_heads=N_HEADS,\n",
    "    forward_dim=FORWARD_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/262 [00:00<?, ?it/s]/Users/andrea/Desktop/crispy-fortnight/transformer.py:186: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4412.)\n",
      "  K_transposed = K.T.permute(3, 1, 0, 2)\n",
      "Epoch [1/30], Loss: 0.6764: 100%|██████████| 262/262 [00:11<00:00, 22.04it/s]\n",
      "Epoch [2/30], Loss: 0.5681: 100%|██████████| 262/262 [00:11<00:00, 23.24it/s]\n",
      "Epoch [3/30], Loss: 0.5263: 100%|██████████| 262/262 [00:12<00:00, 21.61it/s]\n",
      "Epoch [4/30], Loss: 0.4217: 100%|██████████| 262/262 [00:11<00:00, 22.38it/s]\n",
      "Epoch [5/30], Loss: 0.4353: 100%|██████████| 262/262 [00:11<00:00, 22.92it/s]\n",
      "Epoch [6/30], Loss: 0.3861: 100%|██████████| 262/262 [00:11<00:00, 22.12it/s]\n",
      "Epoch [7/30], Loss: 0.2752: 100%|██████████| 262/262 [00:11<00:00, 23.44it/s]\n",
      "Epoch [8/30], Loss: 0.2896: 100%|██████████| 262/262 [00:11<00:00, 21.89it/s]\n",
      "Epoch [9/30], Loss: 0.3171:  23%|██▎       | 60/262 [00:02<00:07, 26.40it/s]"
     ]
    }
   ],
   "source": [
    "# model = Transformer(\n",
    "#     src_vocab_size=len(word2idx_src),\n",
    "#     tgt_vocab_size=len(word2idx_tgt),\n",
    "#     src_pad_idx=word2idx_src['<PAD>'],\n",
    "#     tgt_pad_idx=word2idx_tgt['<PAD>'],\n",
    "#     emb_dim=EMB_DIM,\n",
    "#     num_layers=N_LAYERS,\n",
    "#     num_heads=N_HEADS,\n",
    "#     forward_dim=FORWARD_DIM,\n",
    "#     dropout=DROPOUT,\n",
    "#     max_len=MAX_LEN,\n",
    "# ).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx_tgt['<PAD>'])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "losses = []\n",
    "accuraacy = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, (src, tgt) in (pbar := tqdm(enumerate(train_loader), total=len(train_loader))):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output = model(src, tgt)\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_epoch_loss)\n",
    "checkpoint_path = f\"transformer_exp1.pth\"\n",
    "torch.save(\n",
    "    {'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the word-to-index mapping\n",
    "idx2word_src = {idx: w for w, idx in word2idx_src.items()}\n",
    "idx2word_tgt = {idx: w for w, idx in word2idx_tgt.items()}\n",
    "\n",
    "def decode_indices(indices, idx2word):\n",
    "    return ' '.join(idx2word[idx] for idx in indices if idx in idx2word and idx != word2idx_src['<PAD>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKEN LEVEL ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ckp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mcheckpoint_path\u001b[49m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_path' is not defined"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "checkpoint_path = f\"transformer_exp1.pth\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # output = model(src, tgt)  # Output predictions for target tokens\n",
    "        output = model.forward(src)\n",
    "        predictions = output.argmax(dim=-1)  # Get most probable token indices\n",
    "\n",
    "        # Decode result\n",
    "        src_indices = src[0].tolist()\n",
    "        tgt_indices = tgt[0].tolist()\n",
    "        pred_indices = predictions[0].tolist()\n",
    "        decoded_src = decode_indices(src_indices, idx2word_src)\n",
    "        decoded_tgt = decode_indices(tgt_indices, idx2word_tgt)\n",
    "        decoded_pred = decode_indices(pred_indices, idx2word_tgt)\n",
    "\n",
    "        print(f\"Source    : {decoded_src}\")\n",
    "        print(f\"Target    : {decoded_tgt}\")\n",
    "        print(f\"Prediction: {decoded_pred}\")\n",
    "\n",
    "        # Exclude padding tokens from evaluation\n",
    "        mask = (tgt != word2idx_tgt['<PAD>'])\n",
    "        total_tokens += mask.sum().item()\n",
    "        correct_tokens += ((predictions == tgt) & mask).sum().item()\n",
    "\n",
    "accuracy = correct_tokens / total_tokens * 100\n",
    "print(f\"Token-Level Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([[ 1,  9,  7,  2,  3,  1,  9, 14,  8,  0]])\n",
      "ground t: tensor([[5, 8, 8, 8, 8, 8, 8, 7, 7, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "predicted: tensor([[5, 7, 7, 7, 7, 7, 7, 8]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/3h14zrzj5xq281wzb60xyh9c0000gn/T/ipykernel_13724/3834421175.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckp = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f\"transformer_exp1.pth\"\n",
    "ckp = torch.load(checkpoint_path)\n",
    "model.load_state_dict(ckp['model_state_dict'])\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "def inference(input_seq: str):\n",
    "    model.eval()\n",
    "    pass\n",
    "\n",
    "model.eval()\n",
    "\n",
    "src, true_tgt = next(iter(test_loader))\n",
    "print(src.shape)\n",
    "src = src[0].unsqueeze(0).to(device)\n",
    "true_tgt = true_tgt[0].unsqueeze(0).to(device)\n",
    "# src, true_tgt = next(iter(test_loader))[0][0].unsqueeze(0).to(device)\n",
    "\n",
    "print(src)\n",
    "tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "# tgt = torch.tensor([[word2idx_tgt['<SOS>']]]).to(device)\n",
    "\n",
    "# print(f'tgt {tgt.shape}')\n",
    "# print(f'src {src.shape}')\n",
    "iterations = 20\n",
    "pred_sequence = [tgt.item()]\n",
    "# print(pred_sequence)\n",
    "\n",
    "for i in range(iterations):\n",
    "    with torch.no_grad():\n",
    "        # print(tgt)\n",
    "        output = model.forward(src, tgt)\n",
    "        predictions = nn.functional.softmax(output[:, -1, :], dim=-1)\n",
    "        # print(\"argmax:\", predictions.argmax(-1).shape)\n",
    "        next_token = predictions.argmax(-1).item()\n",
    "\n",
    "        if next_token == word2idx_tgt['<EOS>']:\n",
    "            break\n",
    "\n",
    "        next_token_tensor = torch.tensor([[next_token]]).to(device)\n",
    "        tgt = torch.cat([tgt, next_token_tensor], dim=1)\n",
    "        \n",
    "        # pred_sequence.append(next_token)\n",
    "        # tgt = torch.tensor(pred_sequence).unsqueeze(0).to(device)\n",
    "        # print(tgt.shape)\n",
    "        # print(\"tgt\", tgt)\n",
    "\n",
    "print(f'ground t: {true_tgt}')\n",
    "print(f'predicted: {tgt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
